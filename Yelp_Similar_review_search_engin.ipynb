{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Yelp Data Challenge - NLP\n",
    "\n",
    "BitTiger DS501\n",
    "\n",
    "Jun 2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('last_2_years_restaurant_reviews.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>name</th>\n",
       "      <th>categories</th>\n",
       "      <th>avg_stars</th>\n",
       "      <th>review_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "      <th>useful</th>\n",
       "      <th>funny</th>\n",
       "      <th>cool</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>--9e1ONYQuAa-CB_Rrw7Tw</td>\n",
       "      <td>\"Delmonico Steakhouse\"</td>\n",
       "      <td>Cajun/Creole;Steakhouses;Restaurants</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6SgvNWJltnZhW7duJgZ42w</td>\n",
       "      <td>oFyOUOeGTRZhFPF9uTqrTQ</td>\n",
       "      <td>5</td>\n",
       "      <td>2016-03-31</td>\n",
       "      <td>This is mine and my fiancé's favorite steakhou...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>--9e1ONYQuAa-CB_Rrw7Tw</td>\n",
       "      <td>\"Delmonico Steakhouse\"</td>\n",
       "      <td>Cajun/Creole;Steakhouses;Restaurants</td>\n",
       "      <td>4.0</td>\n",
       "      <td>UxFpgng8dPMWOj99653k5Q</td>\n",
       "      <td>aVOGlN9fZ-BXcbtj6dbf0g</td>\n",
       "      <td>5</td>\n",
       "      <td>2016-02-10</td>\n",
       "      <td>Truly Fantastic!  Best Steak ever. Service was...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              business_id                    name  \\\n",
       "0  --9e1ONYQuAa-CB_Rrw7Tw  \"Delmonico Steakhouse\"   \n",
       "1  --9e1ONYQuAa-CB_Rrw7Tw  \"Delmonico Steakhouse\"   \n",
       "\n",
       "                             categories  avg_stars               review_id  \\\n",
       "0  Cajun/Creole;Steakhouses;Restaurants        4.0  6SgvNWJltnZhW7duJgZ42w   \n",
       "1  Cajun/Creole;Steakhouses;Restaurants        4.0  UxFpgng8dPMWOj99653k5Q   \n",
       "\n",
       "                  user_id  stars        date  \\\n",
       "0  oFyOUOeGTRZhFPF9uTqrTQ      5  2016-03-31   \n",
       "1  aVOGlN9fZ-BXcbtj6dbf0g      5  2016-02-10   \n",
       "\n",
       "                                                text  useful  funny  cool  \n",
       "0  This is mine and my fiancé's favorite steakhou...       0      0     0  \n",
       "1  Truly Fantastic!  Best Steak ever. Service was...       0      0     0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The feature variables is the text of the review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take the values of the column that contains review text data, save to a variable named \"documents\"\n",
    "documents = df['text'].values # here .values makes result numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(dtype('O'), (348455,))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# inspect your documents, e.g. check the size, take a peek at elements of the numpy array\n",
    "documents.dtype, documents.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'It\\'s hard for me to only give four stars but....  I ate at CUT WHICH IS a two minute walk from Delmonicos two nights ago.   The service was great the food was good but CUT is a masterpiece the Mona Lisa.  I ordered the spinach salad which was very good fresh and delicious.  For my main course I ordered the bone in rib eye \"house specialty\"  it was great but it did not compare to the magic of my CUT ribeye.  This is an excellent restaurant the only flaw is its two minutes away from CUT!  It\\'s like playing basketball next to Jordan baseball next to Ruth.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split to documents_train, documents_test, target_train, target_test\n",
    "documents_train, documents_test, target_train, target_test = train_test_split(\n",
    "    documents,\n",
    "    target,\n",
    "    test_size = 0.2,\n",
    "    random_state = 42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's get NLP representation of the documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I used the TFidf to vectorize the features, and select out all the words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create TfidfVectorizer, and name it vectorizer\n",
    "vectorizer = TfidfVectorizer(stop_words = 'english', max_features=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model with your training data\n",
    "vectors_train = vectorizer.fit_transform(documents_train).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the vocab of your tfidf\n",
    "words = vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(278764, 5000)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectors_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the trained model to transform your test data\n",
    "vectors_test = vectorizer.transform(documents_test).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similar review search engine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I used the cosine similarity to identify the similarity between the two review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# We will need these helper methods pretty soon\n",
    "\n",
    "def get_top_values(lst, n, labels):\n",
    "    return [labels[i] for i in np.argsort(lst)[::-1][:n]]  \n",
    "\n",
    "def get_bottom_values(lst, n, labels):\n",
    "    return [labels[i] for i in np.argsort(lst)[:n]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This place has the best food. Everything is brought as it's made and ready so food is deliciously fresh. Their devil eggs is a must to try. Love love love their jerk turkey burger with tater tots. And please leave room for dessert, their bread pudding is amazing!!! Plus you will love the atmosphere and they just expanded and added a bar area. Recommend the sangria.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "# Draw an arbitrary review from test (unseen in training) documents\n",
    "some_random_number = 42\n",
    "search_query = documents_test[some_random_number]\n",
    "search_queries = [search_query] # Need to be put into a list-like format\n",
    "print(search_query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the drawn review(s) to vector(s)\n",
    "vector_search_queries = vectorizer.transform(search_queries).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the similarity score(s) between vector(s) and training vectors\n",
    "similarity_scores = cosine_similarity(vector_search_queries, vectors_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's find top 3 similar reviews\n",
    "n = 3\n",
    "returned_reviews = get_top_values(similarity_scores[0], n, documents_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our search query:\n",
      "This place has the best food. Everything is brought as it's made and ready so food is deliciously fresh. Their devil eggs is a must to try. Love love love their jerk turkey burger with tater tots. And please leave room for dessert, their bread pudding is amazing!!! Plus you will love the atmosphere and they just expanded and added a bar area. Recommend the sangria.\n"
     ]
    }
   ],
   "source": [
    "print('Our search query:')\n",
    "print(search_queries[0]) # To be added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query\n",
      "This place has the best food. Everything is brought as it's made and ready so food is deliciously fresh. Their devil eggs is a must to try. Love love love their jerk turkey burger with tater tots. And please leave room for dessert, their bread pudding is amazing!!! Plus you will love the atmosphere and they just expanded and added a bar area. Recommend the sangria.\n",
      "\n",
      "\n",
      "Most 3 similar reviews:\n",
      "#0:\n",
      "Always very friendly, service and food is amazing.  I love the takuyaki here!  Love it here\n",
      "#1:\n",
      "Amazing food! I love the customer service and the coffee... Love love love! Must try this spot. Ask for Julia\n",
      "#2:\n",
      "Love love love love love love love cheap fun good food for cheap fun and good love the people and place dearly yessssss !!!!\n"
     ]
    }
   ],
   "source": [
    "print('query')\n",
    "print(search_query)\n",
    "\n",
    "print('\\n\\nMost %s similar reviews:' % n)\n",
    "for i, review in enumerate(returned_reviews):\n",
    "    print('#%s:' % i)\n",
    "    print(review)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see that all the reviews are positive and it wonldn't surprize me that they are similar. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifying positive/negative review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Naive-Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build a Naive-Bayes Classifier\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "model_nb = MultinomialNB()\n",
    "\n",
    "model_nb.fit(vectors_train, target_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8153133116184299"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get score for training set\n",
    "model_nb.score(vectors_train, target_train) # accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.80861947740741269"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get score for test set\n",
    "model_nb.score(vectors_test, target_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build a Logistic Regression Classifier\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model_lrc = LogisticRegression()\n",
    "model_lrc.fit(vectors_train, target_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.84719691208333936"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get score for training set\n",
    "model_lrc.score(vectors_train, target_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.82946147996154451"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get score for test set\n",
    "model_lrc.score(vectors_test, target_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What are the key features(words) that make the positive prediction?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['amazing',\n",
       " 'best',\n",
       " 'delicious',\n",
       " 'awesome',\n",
       " 'thank',\n",
       " 'perfect',\n",
       " 'fantastic',\n",
       " 'great',\n",
       " 'bomb',\n",
       " 'highly',\n",
       " 'phenomenal',\n",
       " 'love',\n",
       " 'favorite',\n",
       " 'gem',\n",
       " 'excellent',\n",
       " 'perfection',\n",
       " 'wonderful',\n",
       " 'incredible',\n",
       " 'heaven',\n",
       " 'perfectly']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's find it out by ranking\n",
    "n = 20\n",
    "get_top_values(model_lrc.coef_[0], n, words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What are the key features(words) that make the negative prediction?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['worst',\n",
       " 'ok',\n",
       " 'horrible',\n",
       " 'mediocre',\n",
       " 'rude',\n",
       " 'terrible',\n",
       " 'disappointing',\n",
       " 'slow',\n",
       " 'bland',\n",
       " 'okay',\n",
       " 'poor',\n",
       " 'average',\n",
       " 'decent',\n",
       " 'overpriced',\n",
       " 'lacking',\n",
       " 'wasn',\n",
       " 'unfortunately',\n",
       " 'dry',\n",
       " 'worse',\n",
       " 'awful']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's find it out by ranking\n",
    "n = 20\n",
    "get_bottom_values(model_lrc.coef_[0], n, words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=20, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=10,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            n_estimators=50, n_jobs=-1, oob_score=False, random_state=None,\n",
       "            verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build a Random Forest Classifier\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "model_rfc = RandomForestClassifier(max_depth = 20,\n",
    "                                   n_estimators = 50,\n",
    "                                   min_samples_leaf = 10,\n",
    "                                   n_jobs = -1)\n",
    "model_rfc.fit(vectors_train, target_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.79988807737010514"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get score for training set\n",
    "model_rfc.score(vectors_train, target_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.78458122282647691"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get score for test set\n",
    "model_rfc.score(vectors_test, target_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### what features (words) are important by inspecting the RFC model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['amazing',\n",
       " 'delicious',\n",
       " 'best',\n",
       " 'love',\n",
       " 'great',\n",
       " 'ok',\n",
       " 'bad',\n",
       " 'didn',\n",
       " 'definitely',\n",
       " 'awesome',\n",
       " 'worst',\n",
       " 'wasn',\n",
       " 'minutes',\n",
       " 'horrible',\n",
       " 'good',\n",
       " 'favorite',\n",
       " 'vegas',\n",
       " 'like',\n",
       " 'place',\n",
       " 'money']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = 20\n",
    "get_top_values(model_rfc.feature_importances_, n, words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.83119306,  0.82294282,  0.82988951,  0.82221266,  0.82349143])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "cv_scores = cross_val_score(model_lrc,\n",
    "                            vectors_train,\n",
    "                            target_train,\n",
    "                            cv = 5,\n",
    "                            scoring=\"accuracy\")\n",
    "cv_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Tuning hyper-parameters for accuracy\n",
      "\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "\n",
      "{'C': 100, 'penalty': 'l2'}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "\n",
      "0.538 (+/-0.005) for {'C': 0.1, 'penalty': 'l1'}\n",
      "0.746 (+/-0.027) for {'C': 100, 'penalty': 'l1'}\n",
      "0.572 (+/-0.047) for {'C': 0.1, 'penalty': 'l2'}\n",
      "0.764 (+/-0.046) for {'C': 100, 'penalty': 'l2'}\n",
      "\n",
      "Detailed classification report:\n",
      "\n",
      "The model is trained on the full development set.\n",
      "The scores are computed on the full evaluation set.\n",
      "\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      False       0.75      0.79      0.77    145336\n",
      "       True       0.76      0.71      0.74    133428\n",
      "\n",
      "avg / total       0.75      0.75      0.75    278764\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Let's tune Logistic Regression Regularization parameter C and different penalty\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "param_grid = [{'penalty':['l1'], 'C':[0.1, 100]},\n",
    "              {'penalty':['l2'], 'C':[0.1, 100]}]\n",
    "\n",
    "scores = ['accuracy']\n",
    "\n",
    "for score in scores:\n",
    "    print(\"# Tuning hyper-parameters for %s\" % score + \"\\n\\n\")\n",
    "    clf = GridSearchCV(LogisticRegression(),\n",
    "                       param_grid,\n",
    "                       cv=5,\n",
    "                       scoring=score)\n",
    "    clf.fit(vectors_train[:500,:], target_train[:500])\n",
    "    print(\"Best parameters set found on development set:\\n\\n\")\n",
    "    print(clf.best_params_)\n",
    "    print(\"\\nGrid scores on development set:\\n\\n\")\n",
    "    means = clf.cv_results_['mean_test_score']\n",
    "    stds = clf.cv_results_['std_test_score']\n",
    "    for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "        print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "              % (mean, std * 2, params))\n",
    "    \n",
    "    print(\"\\nDetailed classification report:\\n\")\n",
    "    print(\"The model is trained on the full development set.\")\n",
    "    print(\"The scores are computed on the full evaluation set.\")\n",
    "    print(\"\\n\")\n",
    "    y_true, y_pred = target_test, clf.predict(vectors_test)\n",
    "    print(classification_report(y_true, y_pred))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
